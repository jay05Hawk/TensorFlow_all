{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhy9FWDVBgOW5EuKA8gUwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/TensorFlow_all/blob/main/Tensorflow_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Data pipelines\n",
        "\n",
        "> Data may also be passed into the fit method as a tf.data.Dataset() iterator\n",
        "> The from_tensor_slices() method converts the NumPy arrays into a dataset\n",
        "> The batch() and shuffle() methods chained together. \n",
        "\n",
        ">Next, the map() method invokes a method on the input images, x, that randomly flips one in two of them across\n",
        "the y-axis, effectively increasing the size of the image set\n",
        "\n",
        ">Finally, the repeat() method means that the dataset will be re-fed from the beginning when its end is\n",
        "reached (continuously)"
      ],
      "metadata": {
        "id": "fj1YwKQg1enp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY260qOH1SQH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
        "train_x, test_x = train_x/255.0, test_x/255.0\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "buffer_size = 10000\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
        "training_dataset = training_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "training_dataset = training_dataset.repeat()"
      ],
      "metadata": {
        "id": "lWkKj3Kk1xiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)\n",
        "testing_dataset = training_dataset.repeat()"
      ],
      "metadata": {
        "id": "k3y06X811xfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model architecture"
      ],
      "metadata": {
        "id": "WEKzqLm11-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now in the fit() function, we can pass the dataset directly in, as follows:\n",
        "model5 = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "ZTU7ekfy1xcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling the model"
      ],
      "metadata": {
        "id": "Zz44sQUe2FkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(train_x)//batch_size #required becuase of the repeat() on the dataset\n",
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model5.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "hQKkQB7P1y1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting the model"
      ],
      "metadata": {
        "id": "JvAmWp072L-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch = steps_per_epoch)"
      ],
      "metadata": {
        "id": "iFAezWsf1yur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the model"
      ],
      "metadata": {
        "id": "QI-n116Q2S76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ],
      "metadata": {
        "id": "NjXZsp_51xX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "callbacks = [\n",
        "  # Write TensorBoard logs to `./logs` directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='log/{}/'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
        "]"
      ],
      "metadata": {
        "id": "bZlbpALU1xTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=testing_dataset,\n",
        "          validation_steps=3)"
      ],
      "metadata": {
        "id": "FO1i_xVg1xPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating"
      ],
      "metadata": {
        "id": "vyKC94ux2gIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ],
      "metadata": {
        "id": "JQ2z5J1w1xMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading Keras models\n",
        "\n",
        ">The Keras API in TensorFlow has the ability to save and restore models easily. This is done as follows, and saves the model in the current directory. Of course, a longer path may be passed here:\n",
        "\n",
        "#### Saving a model\n",
        "    \n",
        "`model.save('./model_name.h5')`\n",
        "\n",
        ">This will save the model architecture, its weights, its training state (loss, optimizer), and the state of the optimizer, so that you can carry on training the model from where you left off.\n"
      ],
      "metadata": {
        "id": "eKo9a7Y52mXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Loading a saved model is done as follows. Note that if you have compiled your model, the load will compile your model using the saved training configuration:\n",
        "\n",
        "#### Loding a model\n",
        "\n",
        "`from tensorflow.keras.models import load_model\n",
        "new_model = load_model('./model_name.h5')`"
      ],
      "metadata": {
        "id": "zEjosgPL2tvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">It is also possible to save just the model weights and load them with this (in which case, you must build your architecture to load the weights into):\n",
        "\n",
        "#### Saving the model weights only\n",
        "    \n",
        "    `model.save_weights('./model_weights.h5')`\n",
        "    \n",
        ">Then use the following to load it:\n",
        "\n",
        "#### Loding the weights\n",
        "    \n",
        "    `model.load_weights('./model_weights.h5')`"
      ],
      "metadata": {
        "id": "A8O1ARlw2y4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras datasets\n",
        "\n",
        ">The following datasets are available from within Keras: boston_housing, cifar10, cifar100, fashion_mnist, imdb, mnist,and reuters.\n",
        "\n",
        ">They are all accessed with the function.\n",
        "\n",
        "`load_data()`  \n",
        "\n",
        ">For example, to load the fashion_mnist dataset, use the following:\n",
        "\n",
        "`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()`"
      ],
      "metadata": {
        "id": "jXEh7P5c23SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using NumPy arrays with datasets"
      ],
      "metadata": {
        "id": "cHHEiq1L27yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "number_items = 11\n",
        "number_list1 = np.arange(number_items)\n",
        "number_list2 = np.arange(number_items,number_items*2)"
      ],
      "metadata": {
        "id": "ncrQ5J7P1xJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create datasets, using the from_tensor_slices() method"
      ],
      "metadata": {
        "id": "8AoeW3nT3B36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1)"
      ],
      "metadata": {
        "id": "pu_ImBsB1xGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create an iterator on it using the make_one_shot_iterator() method:"
      ],
      "metadata": {
        "id": "CCle9xTB3Hpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)"
      ],
      "metadata": {
        "id": "evce0x0_1xDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using them together, with the get_next method:"
      ],
      "metadata": {
        "id": "iyB8EtWe3Q0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "qK9JhITj1xAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Note that executing this code twice in the same program run will raise an error because we are using a one-shot iterator\n",
        "\n",
        "\n",
        "#### It's also possible to access the data in batches() with the batch method. Note that the first argument is the number of elements to put in each batch and the second is the self-explanatory drop_remainder argument:"
      ],
      "metadata": {
        "id": "0pjt9Zcq3f2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1).batch(3, drop_remainder = False)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)\n",
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)"
      ],
      "metadata": {
        "id": "kWEN2IMS1w9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There is also a zip method, which is useful for presenting features and labels together:"
      ],
      "metadata": {
        "id": "kptfI_Fh3p_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set1 = [1,2,3,4,5]\n",
        "data_set2 = ['a','e','i','o','u']\n",
        "data_set1 = tf.data.Dataset.from_tensor_slices(data_set1)\n",
        "data_set2 = tf.data.Dataset.from_tensor_slices(data_set2)\n",
        "zipped_datasets = tf.data.Dataset.zip((data_set1, data_set2))\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
        "for item in zipped_datasets:\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "nPKDps8T3lD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can concatenate two datasets as follows, using the concatenate method:"
      ],
      "metadata": {
        "id": "pce1dLrx4CAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
        "datas2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
        "datas3 = datas1.concatenate(datas2)\n",
        "print(datas3)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(datas3)\n",
        "for i in range(14):\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "4vj_FIH03lAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can also do away with iterators altogether, as shown here:"
      ],
      "metadata": {
        "id": "oIl1I5EV4KO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2\n",
        "for e in range(epochs):\n",
        "    for item in datas3:\n",
        "        print(item)\n"
      ],
      "metadata": {
        "id": "-DpC6c9Q3k9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using comma-separated value (CSV)files with datasets.\n",
        "\n",
        ">CSV files are a very popular method of storing data. TensorFlow 2 contains flexible methods for dealing with them. \n",
        "\n",
        ">The main method here is tf.data.experimental.CsvDataset."
      ],
      "metadata": {
        "id": "_-dE0HjC4TSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CSV Example 1\n",
        "\n",
        "\n",
        ">With the following arguments, our dataset will consist of two items taken from each row of the\n",
        "filename file, both of the float type, with the first line of the file ignored and columns 1 and 2 used\n",
        "(column numbering is, of course, 0-based):\n"
      ],
      "metadata": {
        "id": "fv0kpNhi4aqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = [\"./size_1000.csv\"]\n",
        "record_defaults = [tf.float32] * 2 # two required float columns\n",
        "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])\n",
        "for item in data_set:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "IIqZMnwG3kwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #CSV example 2"
      ],
      "metadata": {
        "id": "aAXsQNVE4npd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"mycsvfile.txt\"\n",
        "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
        "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
        "for item in data_set:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "6ngeJzZ53kpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### #CSV example 3\n",
        "\n",
        "\n",
        "#For our final example, our dataset will consist of two required floats and a required string, where the\n",
        "#CSV file has a header variable:\n",
        "filename = \"file1.txt\"\n",
        "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
        "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
        "for item in dataset:\n",
        "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() )\n"
      ],
      "metadata": {
        "id": "l_vFczds3kmZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}