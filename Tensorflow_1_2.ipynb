{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjIa5/9VTrp2LiT9CEmlPv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/TensorFlow_all/blob/main/Tensorflow_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Data pipelines\n",
        "\n",
        "> Data may also be passed into the fit method as a tf.data.Dataset() iterator\n",
        "> The from_tensor_slices() method converts the NumPy arrays into a dataset\n",
        "> The batch() and shuffle() methods chained together. \n",
        "\n",
        ">Next, the map() method invokes a method on the input images, x, that randomly flips one in two of them across\n",
        "the y-axis, effectively increasing the size of the image set\n",
        "\n",
        ">Finally, the repeat() method means that the dataset will be re-fed from the beginning when its end is\n",
        "reached (continuously)"
      ],
      "metadata": {
        "id": "fj1YwKQg1enp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY260qOH1SQH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_x,train_y), (test_x, test_y) = mnist.load_data()\n",
        "train_x, test_x = train_x/255.0, test_x/255.0\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "buffer_size = 10000\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(32).shuffle(10000)\n",
        "training_dataset = training_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "training_dataset = training_dataset.repeat()"
      ],
      "metadata": {
        "id": "lWkKj3Kk1xiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(batch_size).shuffle(10000)\n",
        "testing_dataset = training_dataset.repeat()"
      ],
      "metadata": {
        "id": "k3y06X811xfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model architecture"
      ],
      "metadata": {
        "id": "WEKzqLm11-bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now in the fit() function, we can pass the dataset directly in, as follows:\n",
        "model5 = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(),\n",
        " tf.keras.layers.Dense(512,activation=tf.nn.relu),\n",
        " tf.keras.layers.Dropout(0.2),\n",
        " tf.keras.layers.Dense(10,activation=tf.nn.softmax)\n",
        "])"
      ],
      "metadata": {
        "id": "ZTU7ekfy1xcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compiling the model"
      ],
      "metadata": {
        "id": "Zz44sQUe2FkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = len(train_x)//batch_size #required becuase of the repeat() on the dataset\n",
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model5.compile (optimizer= optimiser, loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "hQKkQB7P1y1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting the model"
      ],
      "metadata": {
        "id": "JvAmWp072L-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch = steps_per_epoch)"
      ],
      "metadata": {
        "id": "iFAezWsf1yur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the model"
      ],
      "metadata": {
        "id": "QI-n116Q2S76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ],
      "metadata": {
        "id": "NjXZsp_51xX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "callbacks = [\n",
        "  # Write TensorBoard logs to `./logs` directory\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='log/{}/'.format(dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
        "]"
      ],
      "metadata": {
        "id": "bZlbpALU1xTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5.fit(training_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=testing_dataset,\n",
        "          validation_steps=3)"
      ],
      "metadata": {
        "id": "FO1i_xVg1xPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating"
      ],
      "metadata": {
        "id": "vyKC94ux2gIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model5.evaluate(testing_dataset,steps=10)"
      ],
      "metadata": {
        "id": "JQ2z5J1w1xMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading Keras models\n",
        "\n",
        ">The Keras API in TensorFlow has the ability to save and restore models easily. This is done as follows, and saves the model in the current directory. Of course, a longer path may be passed here:\n",
        "\n",
        "#### Saving a model\n",
        "    \n",
        "`model.save('./model_name.h5')`\n",
        "\n",
        ">This will save the model architecture, its weights, its training state (loss, optimizer), and the state of the optimizer, so that you can carry on training the model from where you left off.\n"
      ],
      "metadata": {
        "id": "eKo9a7Y52mXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Loading a saved model is done as follows. Note that if you have compiled your model, the load will compile your model using the saved training configuration:\n",
        "\n",
        "#### Loding a model\n",
        "\n",
        "`from tensorflow.keras.models import load_model\n",
        "new_model = load_model('./model_name.h5')`"
      ],
      "metadata": {
        "id": "zEjosgPL2tvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">It is also possible to save just the model weights and load them with this (in which case, you must build your architecture to load the weights into):\n",
        "\n",
        "#### Saving the model weights only\n",
        "    \n",
        "    `model.save_weights('./model_weights.h5')`\n",
        "    \n",
        ">Then use the following to load it:\n",
        "\n",
        "#### Loding the weights\n",
        "    \n",
        "    `model.load_weights('./model_weights.h5')`"
      ],
      "metadata": {
        "id": "A8O1ARlw2y4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keras datasets\n",
        "\n",
        ">The following datasets are available from within Keras: boston_housing, cifar10, cifar100, fashion_mnist, imdb, mnist,and reuters.\n",
        "\n",
        ">They are all accessed with the function.\n",
        "\n",
        "`load_data()`  \n",
        "\n",
        ">For example, to load the fashion_mnist dataset, use the following:\n",
        "\n",
        "`(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()`"
      ],
      "metadata": {
        "id": "jXEh7P5c23SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using NumPy arrays with datasets"
      ],
      "metadata": {
        "id": "cHHEiq1L27yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "number_items = 11\n",
        "number_list1 = np.arange(number_items)\n",
        "number_list2 = np.arange(number_items,number_items*2)"
      ],
      "metadata": {
        "id": "ncrQ5J7P1xJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create datasets, using the from_tensor_slices() method"
      ],
      "metadata": {
        "id": "8AoeW3nT3B36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1)"
      ],
      "metadata": {
        "id": "pu_ImBsB1xGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create an iterator on it using the make_one_shot_iterator() method:"
      ],
      "metadata": {
        "id": "CCle9xTB3Hpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)"
      ],
      "metadata": {
        "id": "evce0x0_1xDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using them together, with the get_next method:"
      ],
      "metadata": {
        "id": "iyB8EtWe3Q0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "qK9JhITj1xAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Note that executing this code twice in the same program run will raise an error because we are using a one-shot iterator\n",
        "\n",
        "\n",
        "#### It's also possible to access the data in batches() with the batch method. Note that the first argument is the number of elements to put in each batch and the second is the self-explanatory drop_remainder argument:"
      ],
      "metadata": {
        "id": "0pjt9Zcq3f2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_list1_dataset = tf.data.Dataset.from_tensor_slices(number_list1).batch(3, drop_remainder = False)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(number_list1_dataset)\n",
        "for item in number_list1_dataset:\n",
        "    number = iterator.get_next().numpy()\n",
        "    print(number)"
      ],
      "metadata": {
        "id": "kWEN2IMS1w9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There is also a zip method, which is useful for presenting features and labels together:"
      ],
      "metadata": {
        "id": "kptfI_Fh3p_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set1 = [1,2,3,4,5]\n",
        "data_set2 = ['a','e','i','o','u']\n",
        "data_set1 = tf.data.Dataset.from_tensor_slices(data_set1)\n",
        "data_set2 = tf.data.Dataset.from_tensor_slices(data_set2)\n",
        "zipped_datasets = tf.data.Dataset.zip((data_set1, data_set2))\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
        "for item in zipped_datasets:\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "nPKDps8T3lD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can concatenate two datasets as follows, using the concatenate method:"
      ],
      "metadata": {
        "id": "pce1dLrx4CAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datas1 = tf.data.Dataset.from_tensor_slices([1,2,3,5,7,11,13,17])\n",
        "datas2 = tf.data.Dataset.from_tensor_slices([19,23,29,31,37,41])\n",
        "datas3 = datas1.concatenate(datas2)\n",
        "print(datas3)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(datas3)\n",
        "for i in range(14):\n",
        "    number = iterator.get_next()\n",
        "    print(number)\n"
      ],
      "metadata": {
        "id": "4vj_FIH03lAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can also do away with iterators altogether, as shown here:"
      ],
      "metadata": {
        "id": "oIl1I5EV4KO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=2\n",
        "for e in range(epochs):\n",
        "    for item in datas3:\n",
        "        print(item)\n"
      ],
      "metadata": {
        "id": "-DpC6c9Q3k9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using comma-separated value (CSV)files with datasets.\n",
        "\n",
        ">CSV files are a very popular method of storing data. TensorFlow 2 contains flexible methods for dealing with them. \n",
        "\n",
        ">The main method here is tf.data.experimental.CsvDataset."
      ],
      "metadata": {
        "id": "_-dE0HjC4TSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CSV Example 1\n",
        "\n",
        "\n",
        ">With the following arguments, our dataset will consist of two items taken from each row of the\n",
        "filename file, both of the float type, with the first line of the file ignored and columns 1 and 2 used\n",
        "(column numbering is, of course, 0-based):\n"
      ],
      "metadata": {
        "id": "fv0kpNhi4aqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = [\"./size_1000.csv\"]\n",
        "record_defaults = [tf.float32] * 2 # two required float columns\n",
        "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=True, select_cols=[1,2])\n",
        "for item in data_set:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "IIqZMnwG3kwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #CSV example 2"
      ],
      "metadata": {
        "id": "aAXsQNVE4npd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"mycsvfile.txt\"\n",
        "record_defaults = [tf.float32, tf.constant([0.0], dtype=tf.float32), tf.int32,]\n",
        "data_set = tf.data.experimental.CsvDataset(filename, record_defaults, header=False, select_cols=[1,2,3])\n",
        "for item in data_set:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "6ngeJzZ53kpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### #CSV example 3\n",
        "\n",
        "\n",
        "#For our final example, our dataset will consist of two required floats and a required string, where the\n",
        "#CSV file has a header variable:\n",
        "filename = \"file1.txt\"\n",
        "record_defaults = [tf.float32, tf.float32, tf.string ,]\n",
        "dataset = tf.data.experimental.CsvDataset(filename, record_defaults, header=False)\n",
        "for item in dataset:\n",
        "    print(item[0].numpy(), item[1].numpy(),item[2].numpy().decode() )\n"
      ],
      "metadata": {
        "id": "l_vFczds3kmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFRecords\n",
        "\n",
        "\n",
        "\n",
        ">TFRecord format is a binary file format. For large files, it is a good choice because binary files take up less disc space, take less time to copy, and can be read very efficiently from the disc. All this can have a significant effect on the efficiency of your data pipeline and thus, the training time of your model. The format is also optimized in a\n",
        "variety of ways for use with TensorFlow. It is a little complex because data has to be converted into\n",
        "the binary format prior to storage and decoded when read back."
      ],
      "metadata": {
        "id": "tqAdXxsB5MQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #TFRecord example 1"
      ],
      "metadata": {
        "id": "DCasi2Ys5SoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">A TFRecord file is a sequence of binary strings, its structure must be specified prior to\n",
        "saving so that it can be properly written and subsequently read back.\n",
        "\n",
        ">TensorFlow has two structures for this, \n",
        "\n",
        "`tf.train.Example and tf.train.SequenceExample. `\n",
        "\n",
        ">We have to store each sample of your data in one of these structures, then serialize it, and use `tf.python_io.TFRecordWriter` to save it to disk.\n",
        "\n",
        ">In the next example, \n",
        "the  data, is first converted to the binary format and then saved to disc.\n",
        "\n",
        ">A feature is a dictionary containing the data that is passed to tf.train.Example prior to serialization and saving the data."
      ],
      "metadata": {
        "id": "nPmyn0Qr5Z9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "data = np.array([10.,11.,12.,13.,14.,15.])\n",
        "def npy_to_tfrecords(fname,data):\n",
        "    writer = tf.io.TFRecordWriter(fname)\n",
        "    feature={}\n",
        "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "    serialized = example.SerializeToString()\n",
        "    writer.write(serialized)\n",
        "    writer.close()\n",
        "npy_to_tfrecords(\"./myfile.tfrecords\",data)"
      ],
      "metadata": {
        "id": "habzCOW65KIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The code to read the record back is as follows. \n",
        "\n",
        ">A parse_function function is constructed that decodes the dataset read back from the file. This requires a dictionary (keys_to_features) with the same name and structure as the saved data:"
      ],
      "metadata": {
        "id": "GULpohOa5jY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QSiJeSrQ5jU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = tf.data.TFRecordDataset(\"./myfile.tfrecords\")\n",
        "def parse_function(example_proto):\n",
        "    keys_to_features = {'data':tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True) }\n",
        "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
        "    return parsed_features['data']\n",
        "data_set = data_set.map(parse_function)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(data_set)\n",
        "# array is retrieved as one item\n",
        "item = iterator.get_next()\n",
        "print(item)\n",
        "print(item.numpy())\n",
        "print(item[2].numpy())"
      ],
      "metadata": {
        "id": "WktdD00d5J_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TFRecord example 2"
      ],
      "metadata": {
        "id": "Ml200JnW5zDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = './students.tfrecords'\n",
        "dataset = {\n",
        "'ID': 61553,\n",
        "'Name': ['Jones', 'Felicity'],\n",
        "'Scores': [45.6, 97.2]}"
      ],
      "metadata": {
        "id": "g9cw3GPk5J7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Using this, we can construct a tf.train.Example class, again using the `Feature()` method. Note how we have to encode our string:\n"
      ],
      "metadata": {
        "id": "AmiFd8KY57cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ID = tf.train.Feature(int64_list=tf.train.Int64List(value=[dataset['ID']]))\n",
        "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in dataset['Name']]))\n",
        "Scores = tf.train.Feature(float_list=tf.train.FloatList(value=dataset['Scores']))\n",
        "example = tf.train.Example(features=tf.train.Features(feature={'ID': ID, 'Name': Name, 'Scores': Scores }))"
      ],
      "metadata": {
        "id": "xUBVpXDW5J4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Serializing and writing this record to disc is the same as TFRecord example 1:"
      ],
      "metadata": {
        "id": "j7gKIqn_6FqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writer_rec = tf.io.TFRecordWriter(filename)\n",
        "writer_rec.write(example.SerializeToString())\n",
        "writer_rec.close()"
      ],
      "metadata": {
        "id": "fsiOZVYh5J0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To read this back, we just need to construct our parse_function function to reflect the structure of the record:"
      ],
      "metadata": {
        "id": "ZPY5OQiK6Ogy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = tf.data.TFRecordDataset(\"./students.tfrecords\")\n",
        "def parse_function(example_proto):\n",
        "    keys_to_features = {'ID':tf.io.FixedLenFeature([], dtype = tf.int64),\n",
        "    'Name':tf.io.VarLenFeature(dtype = tf.string),\n",
        "    'Scores':tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "    parsed_features = tf.io.parse_single_example(serialized=example_proto, features=keys_to_features)\n",
        "    return parsed_features[\"ID\"], parsed_features[\"Name\"],parsed_features[\"Scores\"]"
      ],
      "metadata": {
        "id": "ejNEMvQa5Jw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parsing the data"
      ],
      "metadata": {
        "id": "mUIcSnmF6Y8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data_set.map(parse_function)\n",
        "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "items = iterator.get_next()"
      ],
      "metadata": {
        "id": "N-SR-szO6U1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Record is retrieved as one item"
      ],
      "metadata": {
        "id": "ozVcZ3cC6iL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(items)"
      ],
      "metadata": {
        "id": "_FIhFl1H6Uwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-hot Encoding Example 2"
      ],
      "metadata": {
        "id": "XQvxDug87JPt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iXNH_6966UsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ID: \",item[0].numpy())\n",
        "name = item[1].values.numpy()\n",
        "name1= name[0].decode()\n",
        "name2 = name[1].decode('utf8')\n",
        "print(\"Name:\",name1,\",\",name2)\n",
        "print(\"Scores: \",item[2].values.numpy())"
      ],
      "metadata": {
        "id": "0WNKCjfw6Uom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">Using the fashion MNIST dataset.\n",
        "\n",
        ">The original labels are integers from 0 to 9, so, for example, a label of 5 becomes 0000010000 when onehot encoded, but note the difference between the index and the label stored at that index:"
      ],
      "metadata": {
        "id": "Q621g5Ia7Oyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.datasets import fashion_mnist\n",
        "\n",
        "width, height, = 28,28\n",
        "# total classes\n",
        "n_classes = 10"
      ],
      "metadata": {
        "id": "49YQaeZi6Uji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### loading the dataset"
      ],
      "metadata": {
        "id": "vOIgXe857YME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "YiLtrizb7YIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Split feature training set into training and validation sets\n",
        "\n"
      ],
      "metadata": {
        "id": "0LLHHK2q7XqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split = 50000\n",
        "(y_train, y_valid) = y_train[:split], y_train[split:]"
      ],
      "metadata": {
        "id": "6WUkVGQj7Xm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### one-hot encode the labels using TensorFlow then convert back to numpy for display"
      ],
      "metadata": {
        "id": "WSyj3fh27uOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ohe = tf.one_hot(y_train, depth=n_classes).numpy()\n",
        "y_valid_ohe = tf.one_hot(y_valid, depth=n_classes).numpy()\n",
        "y_test_ohe = tf.one_hot(y_test, depth=n_classes).numpy()\n",
        "\n",
        "# show difference between the original label and a one-hot-encoded label\n",
        "i=8\n",
        "print(y_train[i]) # 'ordinary' number value of label at index i=8 is 5\n",
        "# note the difference between the index of 8 and the label at that index which is 5\n",
        "print(y_train_ohe[i]) "
      ],
      "metadata": {
        "id": "OcWliKGk7XjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ogvn4dly7Wzm"
      }
    }
  ]
}